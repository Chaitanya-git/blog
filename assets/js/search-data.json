{
  
    
        "post0": {
            "title": "SpineNets: A cool new way to design convolutional networks",
            "content": "Introduction . Ever since their introduction in 2012, Convolutional Neural Networks have slowly become the cornerstone for solving computer vision tasks with deep learning. They have worked phenomenally well for several vision-related tasks such as image classification, object detection and instance segmentation. They have since also been applied successfully in Natural Language Processing tasks such as sentiment analysis. . However, the fundamental network architecture of convolutional networks have primarily remained the same, with each layer producing a feature map of increasingly lower resolution, but higher depth. Such an architecture has allowed CNNs to efficiently extract a large number of high-level features from images while keeping resource requirements under check while without compromising performance. . . Such architectures are ‘scale-decreasing’ since they progressively reduce the spatial resolution of the feature map. This makes sense for tasks like image classification that only require the network to report on whether or not an object exists in an image, without having to locate where in the image it is present. For such decisions, extracting high-level features were more important than maintaining spatial information. . In tasks such as object detection, a model must not only recognize objects but must also locate the object in the image. In such scenarios, scale-decreasing architectures cannot directly be applied as they sacrifice spatial information in favour of extracting semantic features. This problem has traditionally been solved using encoder-decoder networks, where an additional decoder network is attached to the scale decreasing network. This decoder tries to recover the lost spatial information to localize an object in an image. . While this has proved to perform well, the authors of the paper argue this is not optimal. They instead propose a scale permuted architecture. The rationale they provide is that for tasks like object detection, the preservation of spatial information is essential to be able to detect the location of an object in an image accurately. A scale permuted architecture will be able to preserve spatial information better by using ‘cross-scale features’. Cross-scale features combine the spatial information from feature maps of different resolutions or scales. Features that are computed at a higher resolution will help detect smaller objects and those computed at lower resolutions can help detect larger objects. In a scale permuted network, the size of feature maps can both increase and decrease with depth, unlike scale decreasing architectures, where increasing the size of feature maps is not allowed. . They back their claims with experiments that show how learning an optimal permutation of the blocks of a standard ResNet-50 using Neural Architecture Search (NAS) can improve its performance. They go on to use NAS to come up with the SpineNet models that achieve better accuracy in object recognition and localization tasks and can match the previous state of the art in image classification while using less compute. . The ‘scale-permuted’ architecture . . The typical scale permuted architecture envisioned by the authors consists of two sections: . A stem network which uses a standard scale decreasing architecture to learn a set of base features. | The scale permuted layers that are stacked on top of the stem network. | The scale permuted layers/blocks are picked from a predetermined list of blocks ${B_1, B_2, ldots, B_n}$. Each of these blocks has an associated level $L_i$. A block at level $L_i$ has a resolution of $1/2^i$ of the input image resolution. Each block can be fed multiple inputs from prior blocks of multiple sizes. Some of these blocks may be connected to one or more layers of the stem network. These connections are termed as cross-scale connections. . In this paper, the models tested set 5 blocks (levels $L_3$ to $L_7$) as output blocks. In contrast, the remaining blocks are designated as intermediate blocks. The output blocks are meant to provide cross-scale features that can then be fed to subnets for classification, object recognition and localization or other vision tasks. Hence, they are each connected to 1x1 Conv layers which ensure that all the output blocks output features (named $P_3, P_4, ldots, P_7$) with the same feature depth. . The permuted ordering of blocks at different scales and the multiple input connections to each block from multiple blocks of potentially different scales allow the network to learn robust multi-scale features. Additionally, the authors also try out certain block adjustments to improve performance. . A closer look at cross-scale connections . In regular scale decreasing convolutional networks, the convolutional layers are designed such that the size of the output feature map of one layer matches the input feature size expected by the subsequent layer. This is ensured by picking the right stride lengths and sometimes by using pooling layers downsample the feature maps as needed. If there are multiple inputs from different layers going into a layer, the inputs are designed to be of the same resolution so that they can be added and sent into the next layer. . But the feature map sizes may not match so easily with scale permuted layers as the subsequent layer may have a wide variety of input resolutions. Providing input from multiple layers is also an issue as the two input features may not be of the same resolution in a scale permuted network. Hence the input features are first processed to have the same input feature length ($ alpha C_{in}$, where $ alpha$ is a scaling parameter), after which spatial resampling (i.e. the spatial resolution is changed) is performed using nearest neighbour upsampling or downsampled using a 3x3 Conv with stride 2 followed by a max-pooling operation. These features are now passed through another 1x1 Conv to make sure that the output feature depth is of a compatible value. The feature vectors are now added together before being sent to the next layer. . . Building (or learning) scale-permuted architectures . A natural complication that arises when trying to build a network with a scale permuted architecture is finding an optimal permutation of blocks to solve the task at hand. Due to a large number of permutations possible within the feature blocks, it’s prohibitively expensive to either hand-design an optimal architecture or try out every permutation in the search space. This problem is alleviated by learning the optimal permutation using Neural Architecture Search. . When using NAS, the entire network architecture is learnt in a three-step process to simplify the procedure: 1. Learn an optimal permutation of feature blocks of different scales. 2. Learn a set of optimal cross-scale connections between the blocks. 3. Optionally optimize the learnt architecture by learning a set of block adjustments for each block. . Search space for learning the optimal architecture . When designing a meta-architecture for a neural network, one needs to ensure that the search space for the learned model architecture is kept reasonably small to reduce the amount of compute and memory resources required to search for or learn the optimal model. This paper identifies and places constraints on three significant factors that decide the size of the search space for learning scale permuted architectures. . Scale permutations . Since a block in any given ordering of blocks can only connect to blocks that come before it in that ordering, the exact arrangement of blocks becomes vital as this decides which cross-scale connections the final network may have. If the scale permuted network had N blocks, this would mean we would have to try out N! different permutations before we could find the optimal ordering. In this paper, the authors significantly reduce the size of the search space by restricting blocks permutations to the set of intermediate and output blocks. In other words, the 5 output blocks and the N-5 intermediate blocks are permuted among themselves. This brings down the size of the search space to (N-5)!5! . Cross-scale connections . If any given block were allowed to receive input from any arbitrary block before it, there would be $2^i$ connection patterns the $i$th block could have. This would mean the number of connection patterns for the entire network would be $2^{N-1}$. The authors thus restrict each block to have only two cross-scale input connections, which significantly brings down the size of the search space to $ displaystyle prod_{i=m}^{N+m-1} {^iC_2}$. . Block adjustments . Since the initially selected list of blocks may not be the optimal choice of blocks to be used when building a scale permuted model, the authors further allow NAS to search through a small set of block adjustment options to improve efficiency. The authors allow two types of adjustments to the scale permuted blocks: . Level adjustment: The authors allow the intermediate blocks to deviate from the current level they are in by {-1, 0, 1, 2}. This results in a search space size of $4^{N-5}$. | Type adjustment: The authors also allow every block to take one of the two types: { bottleneck block (default), residual block }. This results in a search space size of $2^N$. Residual blocks pass on the inputs they have received, along with their outputs. This allows blocks further down the line to utilize features from more than two previous blocks if needed. | Experiments and Results . The authors of the paper test a multitude of models using the scale permuted architecture on Object detection and image classification. The models they test are mainly of two types: . Scale permuted ResNets | Custom SpineNet models that can take full advantage of the scale permuted architecture | Scale permuted ResNet-50 . The authors train and test a few models based on the original ResNet-50 architecture to study the effects of scale permutation on model performance on vision-related tasks. . These models were built by allocating a part of the original ResNet-50 blocks to a scale decreasing stem network and allocating the rest to the scale permuted section of the network. These models are named as R[$N$]-SP[$M$] to indicate that the model has $N$ feature layers in the stem network and $M$ layers in the scale permuted part of the network. A tabulation of the models trained and the block allocations are listed below: . Model Stem network {$L_2, L_3, L_4, L_5$} scale-permuted network {$L_2, L_3, L_4, L_5, L_6, L_7$} . R50 | {3,4,6,3} | {-} | . R35-SP18 | {2,3,5,1} | {1,1,1,1,1,1} | . R23-SP30 | {2,2,2,1} | {1,2,4,1,1,1} | . R14-SP39 | {1,1,1,1} | {2,3,5,1,1,1} | . R0-SP53 | {2,0,0,0} | {1,4,6,2,1,1} | . SpineNet-49 | {2,0,0,0} | {1,2,4,4,2,2} | . By gradually increasing the fraction of the ResNet-50 blocks allocated to the scale permuted sections, the authors study the benefits of the scale permuted architecture. Since they make use of the same feature blocks present in the regular ResNet-50, they ensure that the amount of compute used by the proposed models remains the same and that any improvements seen are solely due to the scale permutation introduced. . The search space for these models only includes models with varying block permutations and cross-scale permutations and block adjustments have been avoided. However, a small adaptation was introduced to the scale permuted models to allow generating multi-scale outputs. The adaption involves removing one $L_5$ block from the set of blocks available and adding an $L_6$ and $L_7$ block and setting the output feature dimensions to 256 for $L_5$, $L_6$ and $L_7$ blocks. These adaptations are necessary to be able to follow the general architecture mentioned earlier. . After testing these models on the object detection on the COCO dataset, the authors observed that as more blocks were allocated to the scale permuted section, the performance of the network improved. The SpineNet-49 model, which was learnt with block adjustments enabled, performed the best as it was able to change the blocks it was using in order to take full advantage of the scale permuted architecture. . This experiment, in my opinion, clearly shows the effectiveness of the scale permuted architecture. . SpineNets . The authors also recognize that the scale permuted ResNets aren’t the best showcase of the potential of a scale permuted architecture as the feature blocks used by ResNets are likely to be suboptimal choices for training scale permuted networks. Therefore, they train another set of models called SpineNets, which were also learnt using NAS. However, while learning these architectures, block adjustment was also introduced into the search space, on top of scale permutation and cross-scale connections. This allows choosing a more optimal set of blocks to take advantage of the scale permuted architecture. . Generating larger SpineNets . The larger SpineNet models are generated from the base SpineNet-49 architecture instead of using NAS to relearn the block permutations and cross-scale connections as using NAS is also computationally expensive. This is done by using a technique called ‘block repeat’. . . The block repeat method works by repeating each feature block in the model multiple times. When a given block $B_k$ is repeated, the repeated instances are connected sequentially. The input going into $B_k$ is connected to the first block in the new group. The output of the last block in the repeated group is sent to all the subsequent blocks that were previously taking the output of $B_k$ as input. . Object detection and instance segmentation . The authors also tested the scale permuted architecture on object detection and instance segmentation tasks by using SpineNets as a backbone model for RetinaNet and Mask-RCNN. Here they once again they found that SpineNets perform better when compared to ResNet-FPNs as they use a scale permuted architecture. . Single-stage object detection with RetinaNet . RetinaNet is a single-stage network for object detection. It utilizes an FPN as a backbone model to extract multi-scale features. It then passes it to two different subnets for classifying objects and locating them. Single-stage detectors often work by sampling many regions within an image as per a pre-defined policy and trying to find objects within them. A large number of regions sampled have no object in them, which makes it harder to train the network due to the imbalance between the individual classes and no object regions. The RetinaNet model handles this by using a focal-loss that increases the weightage given to rare classes, thereby alleviating the imbalance problem. . The authors use a SpineNet as a backbone for a RetinaNet and train the system to recognize objects and draw bounding boxes around them using the popular COCO dataset. Their experiments show that SpineNets outperform other backbone network architectures such as ResNet-FPN and NAS-FPN in both accuracy and efficiency. . Two-stage object detection with Mask-RCNN . Mask-RCNN is a two-stage detector model. It too uses an FPN model as a backbone to generate feature maps from an image. However, it passes these features to two separate networks. The first network scans these features and gives out region proposals or regions of interest. The generated ROI is combined with the image features and given to the second network, which looks at features in the ROIs to predict object classes, bounding boxes and segmentation masks. . In line with the previous experiment, the authors have also trained Mask-RCNN models with various scale-decreasing models and SpineNets as backbones. Here too, SpineNets outperform their scale-decreasing counterparts by a large margin. Moreover, they do so while also being smaller in size. . Real-time object detection . Unlike the previous two experiments, where the performance of SpineNets with its predecessors were compared, here the authors instead quantify the inference latency that can be achieved when using SpineNet models of different sizes in an end-to-end object detection pipeline running with NVIDIA Tensor RT on a V100 GPU. . The end to end pipeline performs additional steps like pre-processing, bounding box and score generation, post-processing and non-maximum suppression along with running the object detection model. The authors were able to run this pipeline in real-time, at over 30 frames per second. . Image classification . Although improving image classification performance was not the primary goal of the SpineNet architecture, the authors have nevertheless tested SpineNet’s performance on image classification. They train and test SpineNet based image classifiers on two datasets, namely the popular ImageNet dataset and a more challenging dataset named the iNaturalist dataset. The iNaturalist dataset is a fine-grained classification dataset that requires networks to detect and classify different species of plants and animals. The dataset is extremely challenging because the classes are imbalanced and different classes are extremely similar visually. The dataset has over 5000 species of plants and animals. . . The experiments showed that SpineNet models can match the performance of a ResNet on the ImageNet dataset while using much fewer FLOPs. On the more challenging iNaturalist dataset SpineNets outperform ResNets by a large margin. . On the iNaturalist dataset, the authors found that there is an almost 5% improvement in Top-1 classification accuracy. To explain this improvement, the authors generate a new dataset from the iNaturalist dataset (named iNaturalist-bbox) in which each image consists of only the object to be detected cropped out and centered in the image, with only a small amount of the original image around it to provide local context. This ensures that all the objects are now visible in the same scale, which means both SpineNets and ResNets should perform equally well when trained to classify on this dataset, if the original improvement was due to SpineNet’s ability to detect objects at different scales. The results obtained are tabulated below. . Model Top-1% Top-5% . SpineNet-49 | 63.9 | 86.9 | . ResNet-50 | 59.6 | 83.3 | . Here we see that even on the modified dataset, SpineNet-49 shows an improvement of approximately 4.3% in Top-1 classification accuracy, even though all the objects are of the same scale. The authors suggest that the classification accuracy may have improved because SpineNet is able to extract fine-grained details from the image with allows to classify between classes that only have very subtle differences and also because it could use a more compact feature representation which makes it more robust against overfitting. . Conclusion . In the SpineNet paper, the authors have identified a problem that exists in the current models that are being used for object detection and related computer vision tasks and have come up with an innovative solution to solve the identified issues. They use thoughtful experiments to show that their architectural improvements do indeed lead to noticeable performance improvement, and show how more optimizations can be made to take full advantage of the scale permuted architecture. . Key Takeaways . To summarize, the key takeaways from the paper are: . Current scale decreasing models are suboptimal for solving computer vision tasks such as object detection where resolution information is essential. | Using the scale permuted architecture introduced by the authors can lead to more optimal networks, that are more efficient and perform better on object detection and related tasks. | The improvements brought about using a scale permuted model translates to tasks like fine-grained image classification, where resolution isn’t important for the final classification, but can still help in extracting fine-grained features. | References . [0] X. Du et al., “SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization”, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. Available: 10.1109/cvpr42600.2020.01161. | [1] T. Lin, P. Goyal, R. Girshick, K. He and P. Dollar, “Focal Loss for Dense Object Detection”, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 42, no. 2, pp. 318-327, 2020. Available: 10.1109/tpami.2018.2858826. | [2] G. Van Horn et al., “The iNaturalist Species Classification and Detection Dataset”, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018. Available: 10.1109/cvpr.2018.00914. | .",
            "url": "https://chaitanya-git.github.io/blog/research/computer%20vision/deep%20learning/2020/09/01/spinenets.html",
            "relUrl": "/research/computer%20vision/deep%20learning/2020/09/01/spinenets.html",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://chaitanya-git.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi! I’m Chaitanya, a college student currently pursuing a Bachelor’s in Computer Science Engineering. I’m a self proclaimed geek, a tinkerer and an open source enthusiast. I’m always looking for new opportunites for using technology to build cool new things that can solve real world problems and help improve the world around me. . . Areas of Interest . Computer Vision | Natural Language Processing | Deep learning | Robotics | Frugal innovation | .",
          "url": "https://chaitanya-git.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "My Projects",
          "content": "Notable Projects . Solo: An autonomous unmanned ground vehicle (UGV) capable of navigating obstacle courses . . Solo is a UGV built by Project Manas for the annual Intelligent Ground Vehicle Competition (IGVC) held at Oakland University, Michigan. Solo helped Project Manas win the IGVC in 2019 and bag many other awards as well. I had loads of fun working on Solo at Project Manas. . SAMIS: A Smartphone based vein imaging system . SAMIS is and android app that uses image processing algorithms to visualize subcutaneous veins in an image of the body of a user taken using a regular smartphone camera. SAMIS utilizes the fact that different wavelengths of light interact in different ways with biological tissue to extract vein information. It is particularly challenging to do on a regular, unmodified smartphone camera because the range of wavelengths of light a smartphone can capture excludes the infrared spectrum, where veins details are simpler to extract. Since IR data is unavailable, SAMIS uses the more subtle variations detectable in the visible spectrum to help visualize vein patterns. SAMIS can potentially be used to help nurses make the process of finding veins for administering Intra-venous injections less error-prone for individuals whose veins are hard to find using conventional methods. SAMIS is potentially useful for diagnosing conditions like varicose veins. Systems like SAMIS, developed on off-the-shelf, easily accessible hardware can bring down the cost of medical diagnostics and allow access to useful tools to doctors and clinics in the less affluent regions of the world. SAMIS was conceptualized and developed by me and my friend Geeve George. SAMIS helped us win the Second Place Grand Award in Biomedical Engineering at Intel International Science And Engineering Fair (ISEF) 2017 at Los Angeles, California. It also helped us win the Grand award at IRIS 2016 science fair, held in Pune. . Magniwear: A Smartphone-based AR/VR microscope . Magniwear is an AR/VR microscope built on the Google Cardboard platform. It allows the user to operate a microscope hands-free using voice commands while wearing it on their head and explore biological samples in a virtual reality environment. This feature makes Magniwear useful as an educational tool, allowing children to explore the microscopic world from a first-person view. It can also function as a dental loupe or as a tool for viewing small samples using the included lens system. Its AR features can help the user highlight useful features better. It also allows streaming what the user sees to other users over the internet, making Magniwear useful during operations other tasks where experts can look at what the user is seeing and provide real-time guidance. I worked on Magniwear with my friend Geeve George and were selected as one of the top 90, globally in the Google Science Fair 2015. . Open Source Software . IntelliDetect: GUI Tool to train and deploy neural networks . Intellidetect is a GUI tool and a neural network library written in C++ using the QT framework. Intellidetect aims to help individual users train neural networks using the GUI and also load neural networks to perform inference on data points later. IntelliDetect allows the user to specify the network architecture in a configuration file, load it using the GUI and point to a dataset for training. Intellidetect can then take care of creating, training and saving the trained network in a portable format, while also providing the user with a rich set of training statistics and visualizations to monitor the training process and to evaluate the performance of the network. The user is allowed the option of loading a pretrained neural network and using that network for prediction tasks. Intellidetect makes this process as easy as clicking on a ‘load’ button, selecting the input from the filesystem and clicking on ‘predict’ to view the prediction within the GUI. I built IntelliDetect with the help of my friend Geeve. . CloneLab: CLI tool to clone GitLab groups . Gitlab allows grouping repos into folder-like structures called groups. While GitLab groups provide a great way to organize a large project having a large number of modules each maintained as a separate git repo, there is no easy way to clone the entire group in which a project resides. CloneLab provides a way to do just that. CloneLab also supports pulling updates to all repos in a GitLab group at once. . ObjectTracker: Kalman-filter based tracking module to track pre-segmented objects in a video stream. . Given a video stream consisting of object masks as input, ObjectTracker tracks the movement of objects using Kalman filters. This module can handle objects moving in and out of the view frame and dynamically tracks the position and velocity of objects. This object tracking module can additionally take care of small amounts of random noise in object positions as well. . Torrid: Heatwave prediction and notification . Heatwaves are an underrated, invisible threat that can potentially kill thousands of unsuspecting people in a single day. Torrid predicts the onset of a Heatwave before it happens by tracking weather data and can detect an adverse response in the user’s body to the extreme heat during a heatwave using a wearable device and automatically call for help if the user is in danger. I built torrid with the help of my friends Dheeraj and Siddharth. . Dental Assessment App: App to automate and digitize the activities of a dental hospital . Hospitals need to file a lot of paperwork and follow a set of procedures tracking the process of providing healthcare to a patient who visits the hospital. A local dental hospital needed to streamline the healthcare delivery process by reducing the effort and paperwork needed to track treatment history, appointments and department visits of incoming patients. As the final project for an iOS course at college I designed and implemented and app for iOS to enable the process with the help of my teammates Arsh and Deepansh. . Hardware Projects . BreatheSafe: Smart indoor air-quality monitor . BreatheSafe is a low-cost indoor air-quality monitor that uses an Arduino Uno and an MQ-35 gas sensor to monitor the levels of CO2 and other VOCs in its vicinity. Monitoring indoor air quality can help identify issues with ventilation and help people breathe safer. This project won a model-making competition held at St. Aloysius College, Mangalore. . Robaldo 2016 bot: Soccer playing bot . Robaldo was a Robo Soccer Challenge Match by PESIT University, Banglore, where we competed against 68 teams from all over India. Our bot was designed from scratch by my friend Geeve and myself. The bot could be controlled wirelessly using a smartphone connected to the Arduino Uno powering the robot via a Bluetooth connection. The bot could be controlled in either direction (i.e. the forward and reverse directions could be flipped flip by pressing a button) which gave us a tactical advantage that let us reach the Quarterfinals and hold our own against much sturdier bots. . Digital Clinometer . A clinometer is a device that uses the principles of trigonometry to measure the height of buildings and other tall objects without having to climb them. The digital clinometer I built uses a Raspberry Pi as the compute unit and interfaces with an ultrasonic sensor for measuring the distance to the object and with an MPU-6050 accelerometer and gyroscope breakout board to help measure the angle of inclination to its top. The Raspberry Pi then uses this information to compute the height of the object. This project was the very first one that I worked on in the 10th grade. It showed me how much fun learning could be! .",
          "url": "https://chaitanya-git.github.io/blog/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://chaitanya-git.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}